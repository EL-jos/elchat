<?php
namespace App\Services;

use App\Models\Site;
use App\Models\Chunk;
use App\Models\Conversation;
use App\Models\Message;
use App\Models\UnansweredQuestion;
use Illuminate\Support\Facades\Http;
use Illuminate\Support\Facades\Log;

class ChatService
{

    public function __construct(
        protected EmbeddingService $embeddingService,
        protected SimilarityService $similarityService
    )
    {}

    /**
     * Traite la question et retourne la r√©ponse factuelle
     */
    public function ask(string $question, Site $site, ?int $topK = 5, float $similarityThreshold = 0.45)
    {
        // 1. G√©n√©rer embedding de la question
        $queryEmbedding = $this->embeddingService->getEmbedding($question);

        // 2. R√©cup√©rer les chunks du site
        $chunks = Chunk::whereHas('page', fn($q) => $q->where('site_id', $site->id))->get();

        // 3. Calculer similarit√© cosine (PHP)
        $chunksWithScore = $chunks->map(fn($chunk) => [
            'chunk' => $chunk,
            'score' => $this->similarityService->cosine(
                $queryEmbedding,
                $chunk->embedding
            )
        ]);

        // 4. Filtrer par score minimum
        $filtered = $chunksWithScore->filter(fn($c) => $c['score'] >= $similarityThreshold);

        // 5. Top K
        $topChunks = $filtered->sortByDesc('score')->take($topK)->pluck('chunk')->toArray();

        // 6. Construire prompt strict pour LLM
        $context = implode("\n", array_map(fn($c) => $c->text, $topChunks));

        // 7. Appel LLM (ici placeholder pour MVP)
        $answer = $this->callLLM($question, $context);

        return $answer;
    }

    /**
     * R√©ponse commerciale incarn√©e (mode production)
     */
    public function answer(Site $site, string $question, Conversation $conversation): string
    {

        $history = Message::where('conversation_id', $conversation->id)
            ->orderBy('created_at', 'desc')
            ->skip(1)
            ->take(6)
            ->get()
            ->reverse()
            ->map(function ($m) {
                if ($m->role === 'bot') {
                    return [
                        'role' => 'assistant',
                        'content' => '[R√©ponse pr√©c√©dente donn√©e au client]',
                    ];
                }

                return [
                    'role' => 'user',
                    'content' => $m->content,
                ];
            })
            ->toArray();

        // 1Ô∏è‚É£ Embedding de la question
        $questionEmbedding = $this->embeddingService->getEmbedding($question);

        // 2Ô∏è‚É£ Charger les chunks du site
        $chunks = Chunk::whereHas('page', fn ($q) =>
        $q->where('site_id', $site->id)
        )->get();

        $scored = [];

        foreach ($chunks as $chunk) {
            $score = $this->similarityService->cosine(
                $questionEmbedding,
                $chunk->embedding
            );

            if ($score >= 0.30) {
                $scored[] = [
                    'text' => $chunk->text,
                    'score' => $score,
                ];
            }
        }

        // 3Ô∏è‚É£ Construire le contexte
        if (empty($scored)) {
            UnansweredQuestion::create([
                'site_id' => $site->id,
                'question' => $question,
            ]);

            // ‚ö†Ô∏è Fallback HUMAIN (cl√© de l‚Äôillusion)
            $context = "Nous n'avons pas communiqu√© publiquement cette information pour le moment.";
        } else {
            usort($scored, fn ($a, $b) => $b['score'] <=> $a['score']);

            $context = collect(array_slice($scored, 0, 3))
                ->pluck('text')
                ->implode("\n\n---\n\n");
        }

        $isSelectionQuestion = preg_match('/moins cher|meilleur|choisir|recommander|quel/i', $question);
        if ($isSelectionQuestion && empty($scored)) {
            $context = "Nous proposons plusieurs produits, mais nous ne communiquons pas de classement par prix.";
        }


        return $this->callLLM($site, $question, $context, $history);
    }

    /**
     * Appel LLM avec PERSONA EMPLOY√â INTERNE
     */
    private function callLLM(Site $site, string $question, string $context, array $history): string
    {
        $companyName = $site->name ?? parse_url($site->url, PHP_URL_HOST);

        $isBuyingIntent = preg_match('/prix|acheter|commander|disponible|livraison/i', $question);
        $isComparison = preg_match('/compar|diff√©rence|vs|meilleur/i', $question);
        $isConcern = preg_match('/dur√©e|garantie|retour|risque|qualit√©/i', $question);


        $systemPrompt = <<<PROMPT
        Tu es un employ√© r√©el de l'entreprise "{$companyName}".

        R√àGLES STRICTES :
        - Tu parles √† la PREMI√àRE PERSONNE (nous / chez nous / notre √©quipe).
        - Tu ne mentionnes JAMAIS :
          - le mot "contexte"
          - le site web
          - une source
          - une analyse
          - une intelligence artificielle
        - Tu r√©ponds comme si tu travaillais r√©ellement dans l'entreprise.
        - Ton ton est professionnel, naturel, commercial et rassurant.
        - Tu n'inventes JAMAIS une caract√©ristique technique, un prix ou une garantie.
        - Tu peux reformuler, expliquer ou valoriser, mais jamais cr√©er une information factuelle.
        - Si intention d‚Äôachat : rassure et incite √† passer √† l‚Äôaction
        - Si h√©sitation : rassure
        - Si comparaison : valorise sans d√©nigrer
        - Termine si possible par une proposition d‚Äôaide naturelle (sans forcer la vente).
        - Tu ne fais jamais de promesse engageante (r√©sultat garanti, effet certain, engagement contractuel).
        - Si la conversation est d√©j√† entam√©e, tu ne recommences jamais par une formule de salutation.

        R√àGLE ABSOLUE SUR LA CONVERSATION :
        - Les messages pr√©c√©dents servent UNIQUEMENT √† comprendre le besoin du client.
        - Les informations factuelles doivent PROVENIR EXCLUSIVEMENT des "Informations internes".
        - Si une information n‚Äôest PAS pr√©sente dans les informations internes, tu dois :
          - rester g√©n√©ral
          - ou proposer d‚Äôaider autrement
        - Tu ne dois JAMAIS d√©duire un produit, une offre ou un prix √† partir d‚Äôune r√©ponse pr√©c√©dente.
        INTERDICTION ABSOLUE :
        - Tu ne dois JAMAIS citer un nom de produit, pack ou offre
          s‚Äôil n‚Äôappara√Æt PAS explicitement mot pour mot
          dans les Informations internes.

        R√îLE :
        Conseiller commercial / employ√© de l‚Äôentreprise.
        PROMPT;

        $userPrompt = <<<PROMPT
        Informations internes √† utiliser si pertinentes :
        {$context}

        Question d‚Äôun client :
        {$question}

        R√©ponds directement au client, comme si tu lui parlais en face.

        Type de demande :
        - Si la question concerne un PRODUIT ‚Üí mets en avant ses b√©n√©fices.
        - Si elle concerne un SERVICE ‚Üí explique l‚Äôaccompagnement.
        - Si elle est G√âN√âRALE ‚Üí rassure et oriente.

        Signal d√©tect√© :
        - Intention d‚Äôachat : {$isBuyingIntent}
        - Comparaison : {$isComparison}
        - Inqui√©tude : {$isConcern}
        PROMPT;

        $messages = [
            ['role' => 'system', 'content' => $systemPrompt],
        ];

        // üß† m√©moire conversationnelle
        foreach ($history as $msg) {
            $messages[] = $msg;
        }

        // question actuelle (avec contexte RAG)
        $messages[] = [
            'role' => 'user',
            'content' => $userPrompt,
        ];

        $response = Http::withHeaders([
            'Authorization' => 'Bearer ' . env('OPENROUTER_API_KEY'),
        ])->post('https://openrouter.ai/api/v1/chat/completions', [
            'model' => 'meta-llama/llama-3.1-8b-instruct',
            'messages' => $messages,
            'temperature' => 0.6,
            'max_tokens' => 350,
        ]);

        return $response->json()['choices'][0]['message']['content']
            ?? "N'h√©sitez pas √† nous contacter, nous serons ravis de vous aider.";
    }
}
