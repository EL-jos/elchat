<?php
namespace App\Services;

use App\Models\Site;
use App\Models\Chunk;
use App\Models\Conversation;
use App\Models\Message;
use App\Models\UnansweredQuestion;
use Illuminate\Support\Facades\Http;
use Illuminate\Support\Facades\Log;

class ChatService
{

    public function __construct(
        protected EmbeddingService $embeddingService,
        protected SimilarityService $similarityService
    )
    {}

    /**
     * Traite la question et retourne la rÃ©ponse factuelle
     */
    public function ask(string $question, Site $site, ?int $topK = 5, float $similarityThreshold = 0.45)
    {
        // 1. GÃ©nÃ©rer embedding de la question
        $queryEmbedding = $this->embeddingService->getEmbedding($question);

        // 2. RÃ©cupÃ©rer les chunks du site
        $chunks = Chunk::whereHas('page', fn($q) => $q->where('site_id', $site->id))->get();

        // 3. Calculer similaritÃ© cosine (PHP)
        $chunksWithScore = $chunks->map(fn($chunk) => [
            'chunk' => $chunk,
            'score' => $this->similarityService->cosine(
                $queryEmbedding,
                $chunk->embedding
            )
        ]);

        // 4. Filtrer par score minimum
        $filtered = $chunksWithScore->filter(fn($c) => $c['score'] >= $similarityThreshold);

        // 5. Top K
        $topChunks = $filtered->sortByDesc('score')->take($topK)->pluck('chunk')->toArray();

        // 6. Construire prompt strict pour LLM
        $context = implode("\n", array_map(fn($c) => $c->text, $topChunks));

        // 7. Appel LLM (ici placeholder pour MVP)
        $answer = $this->callLLM($question, $context);

        return $answer;
    }

    /**
     * RÃ©pond Ã  une question avec un style commercial et fluide
     */
    public function answer(Site $site, string $question): string
    {
        // 1ï¸âƒ£ Embedding de la question
        $questionEmbedding = $this->embeddingService->getEmbedding($question);

        // 2ï¸âƒ£ RÃ©cupÃ©rer les chunks du site
        $chunks = Chunk::whereHas('page', fn($q) => $q->where('site_id', $site->id))
            ->get();

        $scored = [];

        foreach ($chunks as $chunk) {
            $score = $this->similarityService->cosine(
                $questionEmbedding,
                $chunk->embedding
            );

            if ($score >= 0.3) { // seuil plus bas pour attraper plus d'infos
                $scored[] = [
                    'text' => $chunk->text,
                    'score' => $score,
                ];
            }
        }

        // ðŸ”¹ Log debug
        Log::info('RAG DEBUG', [
            'question' => $question,
            'chunks_count' => $chunks->count(),
            'top_scores' => collect($chunks)->map(fn($c) =>
            $this->similarityService->cosine($questionEmbedding, $c->embedding)
            )->sortDesc()->take(5)->values()
        ]);

        // 3ï¸âƒ£ Si aucun chunk pertinent â†’ fallback
        if (empty($scored)) {
            UnansweredQuestion::create([
                'site_id' => $site->id,
                'question' => $question,
            ]);

            // On met un contexte gÃ©nÃ©rique pour que l'IA crÃ©e une rÃ©ponse persuasive
            $context = "Aucune information exacte n'est disponible sur le site pour cette question.";
        } else {
            // 4ï¸âƒ£ Trier par score et limiter top 3
            usort($scored, fn($a, $b) => $b['score'] <=> $a['score']);
            $context = collect(array_slice($scored, 0, 3))
                ->pluck('text')
                ->implode("\n\n---\n\n");
        }

        // 5ï¸âƒ£ Appel LLM commercial
        return $this->callLLM($question, $context);
    }

    /**
     * Appel LLM (OpenRouter/OpenAI)
     * Appel LLM pour rÃ©ponse commerciale fluide
     */
    private function callLLM(string $question, string $context): string
    {
        $response = Http::withHeaders([
            'Authorization' => 'Bearer ' . env('OPENROUTER_API_KEY'),
        ])->post('https://openrouter.ai/api/v1/chat/completions', [
            'model' => 'meta-llama/llama-3.1-8b-instruct', // ou gpt-3.5-turbo si prÃ©fÃ©rÃ©
            'messages' => [
                [
                    'role' => 'system',
                    'content' =>
                        "Tu es un commercial expert. Utilise le CONTEXTE fourni pour rÃ©pondre Ã  la question. "
                        . "MÃªme si le CONTEXTE n'a pas l'information exacte, crÃ©e une rÃ©ponse fluide, persuasive et commerciale."
                ],
                [
                    'role' => 'user',
                    'content' =>
                        "CONTEXTE:\n{$context}\n\nQUESTION:\n{$question}"
                ]
            ],
            'temperature' => 0.7, // plus crÃ©atif et commercial
            'max_tokens' => 400,
        ]);

        return $response->json()['choices'][0]['message']['content']
            ?? "Je ne trouve pas cette information sur ce site.";
    }
}
